# -*- coding: utf-8 -*-
"""LSTM Neuron Network Quant Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z7fhLHenmyUpEa9ZOx2NN9i2ELwDU_2j
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import yfinance as yf

# Set random seed for reproducibility
torch.manual_seed(42)
np.random.seed(42)

# Define LSTM model
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # Initialize hidden state and cell state
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)

        # Forward propagate LSTM
        out, _ = self.lstm(x, (h0, c0))

        # Use only the output of the last time step
        out = self.fc(out[:, -1, :])
        return out

# Define dataset class
class TimeSeriesDataset(Dataset):
    def __init__(self, X, y):
        self.X = X
        self.y = y

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# Function to create time series sequences
def create_sequences(data, seq_length):
    xs, ys = [], []
    for i in range(len(data) - seq_length):
        x = data[i:i+seq_length]
        y = data[i+seq_length]
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)

# Function to train the model
def train_lstm_model(dataloader, model, criterion, optimizer, num_epochs=100):
    model.train()
    losses = []

    for epoch in range(num_epochs):
        epoch_loss = 0
        for X_batch, y_batch in dataloader:
            # Forward pass
            y_pred = model(X_batch)

            # Compute loss
            loss = criterion(y_pred, y_batch)

            # Backward pass and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        # Record average loss
        avg_loss = epoch_loss / len(dataloader)
        losses.append(avg_loss)

        if (epoch+1) % 10 == 0:
            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')

    return losses

# Function to evaluate the model
def evaluate_model(model, X_test, y_test):
    model.eval()
    with torch.no_grad():
        y_pred = model(X_test)
        return y_pred

# Backtest the model
def backtest_model(model, scaler, X_test, y_test):
    """
    Backtest the model, compute error metrics, and visualize results
    """
    # Evaluate the model
    y_pred = evaluate_model(model, X_test, y_test)

    # Inverse transform the predictions
    y_pred_np = y_pred.numpy().reshape(-1, 1)
    y_test_np = y_test.numpy().reshape(-1, 1)

    y_pred_actual = scaler.inverse_transform(y_pred_np)
    y_test_actual = scaler.inverse_transform(y_test_np)

    # Compute evaluation metrics
    mse = np.mean((y_pred_actual - y_test_actual) ** 2)
    rmse = np.sqrt(mse)
    mae = np.mean(np.abs(y_pred_actual - y_test_actual))

    print(f"Backtest MSE: {mse:.4f}")
    print(f"Backtest RMSE: {rmse:.4f}")
    print(f"Backtest MAE: {mae:.4f}")

    # Visualize backtest results
    plt.figure(figsize=(12, 6))
    plt.plot(y_test_actual, label='Actual')
    plt.plot(y_pred_actual, label='Predicted')
    plt.title('Backtest: Prediction vs Actual')
    plt.xlabel('Time Step')
    plt.ylabel('Value')
    plt.legend()
    plt.show()

    return y_test_actual, y_pred_actual

# Use the trained model to predict future values
def predict_future(model, scaler, last_sequence, future_steps=30):
    model.eval()

    # Ensure the input shape is correct
    current_sequence = last_sequence.copy()
    predictions = []

    with torch.no_grad():
        for _ in range(future_steps):
            # Prepare input
            x = torch.FloatTensor(current_sequence).unsqueeze(0)  # Shape: [1, seq_length, input_size]

            # Initialize hidden state and cell state
            h0 = torch.zeros(model.num_layers, 1, model.hidden_size)  # Shape: [num_layers, 1, hidden_size]
            c0 = torch.zeros(model.num_layers, 1, model.hidden_size)  # Shape: [num_layers, 1, hidden_size]

            # Predict the next time step
            y_pred, _ = model.lstm(x, (h0, c0))  # Use LSTM's hidden state and cell state
            y_pred = model.fc(y_pred[:, -1, :])  # Use only the output of the last time step

            # Add to predictions list
            predictions.append(y_pred.item())

            # Update sequence (remove the earliest value, add the new prediction)
            current_sequence = np.append(current_sequence[1:], y_pred.item())

    # Inverse transform the predictions
    predictions = np.array(predictions).reshape(-1, 1)
    predictions = scaler.inverse_transform(predictions)

    return predictions.flatten()

if __name__ == "__main__":
    # 1. Fetch Tesla stock data for the past three years
    ticker = 'TSLA'
    data = yf.download(ticker, start='2022-01-01', end='2025-01-01')

    # 2. Preprocess the data
    scaler = MinMaxScaler()
    data_scaled = scaler.fit_transform(data[['Close']].values)

    # 3. Create sequences
    seq_length = 10
    X, y = create_sequences(data_scaled, seq_length)

    # 4. Split into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # 5. Convert to PyTorch tensors
    X_train = torch.FloatTensor(X_train)
    y_train = torch.FloatTensor(y_train)
    X_test = torch.FloatTensor(X_test)
    y_test = torch.FloatTensor(y_test)

    # 6. Create data loader
    train_dataset = TimeSeriesDataset(X_train, y_train)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

    # 7. Initialize the model
    input_size = 1
    hidden_size = 64
    num_layers = 2
    output_size = 1

    model = LSTMModel(input_size, hidden_size, num_layers, output_size)

    # 8. Define loss function and optimizer
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # 9. Train the model
    train_lstm_model(train_loader, model, criterion, optimizer, num_epochs=100)

    # 10. Backtest the model
    y_test_actual, y_pred_actual = backtest_model(model, scaler, X_test, y_test)

    # 11. Predict future 30 days of stock prices
    last_known_sequence = data['Close'].values[-10:]
    last_known_sequence = scaler.transform(last_known_sequence.reshape(-1, 1))

    future_predictions = predict_future(model, scaler, last_known_sequence, future_steps=30)

    # 12. Visualize future predictions
    plt.figure(figsize=(10, 5))
    plt.plot(future_predictions, label='Predicted Future Prices')
    plt.title('Tesla Stock Price: Future 30 Days Prediction')
    plt.xlabel('Future Time Steps')
    plt.ylabel('Predicted Value')
    plt.legend()
    plt.grid(True)
    plt.show()

# 保存模型
torch.save(model.state_dict(), 'lstm_model.pth')